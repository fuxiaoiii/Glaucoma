{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWUsfk5bsD4N"
   },
   "source": [
    "# This notebook was adapted from this [repo](https://github.com/seva100/optic-nerve-cnn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqiDtrijb-do",
    "outputId": "9148d72f-9286-4918-b386-4926303a1c64",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!module load cuda-11.2\n",
    "!pip install mahotas\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "import mahotas as mh\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, \\\n",
    "    Conv2D, MaxPooling2D, ZeroPadding2D, Input, Embedding, \\\n",
    "    Lambda, UpSampling2D, Cropping2D, Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, CSVLogger\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transf\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GtOZQx1YcAwB",
    "outputId": "3d969476-dde2-4363-ce93-b37bce8ecdf7"
   },
   "outputs": [],
   "source": [
    "!unzip -q './drive/MyDrive/airogs/train_512_0.zip'\n",
    "!unzip -q './drive/MyDrive/airogs/train_512_1.zip'\n",
    "!unzip -q './drive/MyDrive/airogs/train_512_2.zip'\n",
    "!unzip -q './drive/MyDrive/airogs/train_512_3.zip'\n",
    "!unzip -q './drive/MyDrive/airogs/train_512_4.zip'\n",
    "!unzip -q './drive/MyDrive/airogs/train_512_5.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVOXqX-sMeTj"
   },
   "source": [
    "### Training of modified U-Net for Optic Disc on RIM-ONE v3 database, 256 px images (cross-validation fold #0).\n",
    "\n",
    "You can either train your model or upload a pre-trained one from:\n",
    "*../models_weights/03.03,14:19,U-Net light, on RIM-ONE v3 256 px fold 0, SGD, high augm, CLAHE, log_dice loss/last_checkpoint.hdf5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKhQdHHuMeTq",
    "outputId": "31599705-f958-4095-fd28-eb8f68da3c4a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('Keras version:', keras.__version__)\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZrM-57GMeTq"
   },
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn5TuFANMeTt"
   },
   "outputs": [],
   "source": [
    "def tf_to_th_encoding(X):\n",
    "    return np.rollaxis(X, 3, 1)\n",
    "\n",
    "\n",
    "def th_to_tf_encoding(X):\n",
    "    return np.rollaxis(X, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsMjnQYHMeTt"
   },
   "source": [
    "### U-Net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj--6J8TMeTu"
   },
   "outputs": [],
   "source": [
    "def get_unet_light(img_rows=256, img_cols=256):\n",
    "    inputs = Input(( 3,img_rows, img_cols))\n",
    "    conv1 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Dropout(0.3)(conv1)\n",
    "    conv1 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Dropout(0.3)(conv2)\n",
    "    conv2 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Dropout(0.3)(conv3)\n",
    "    conv3 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = Dropout(0.3)(conv4)\n",
    "    conv4 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = Dropout(0.3)(conv5)\n",
    "    conv5 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv5), conv4])\n",
    "    conv6 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up6)\n",
    "    conv6 = Dropout(0.3)(conv6)\n",
    "    conv6 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv6), conv3])\n",
    "    conv7 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up7)\n",
    "    conv7 = Dropout(0.3)(conv7)\n",
    "    conv7 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv7), conv2])\n",
    "    conv8 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(up8)\n",
    "    conv8 = Dropout(0.3)(conv8)\n",
    "    conv8 = Conv2D(64, kernel_size=3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = Concatenate(axis=1)([UpSampling2D(size=(2, 2))(conv8), conv1])\n",
    "    conv9 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(up9)\n",
    "    conv9 = Dropout(0.3)(conv9)\n",
    "    conv9 = Conv2D(32, kernel_size=3, activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(1, kernel_size=1, activation='sigmoid', padding='same')(conv9)\n",
    "    #conv10 = Flatten()(conv10)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xzVKLWqXMeTu",
    "outputId": "d81dd9e1-db5f-42b1-d6da-7f770ff546eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_unet_light(img_rows=256, img_cols=256)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPaceoPgMeTu"
   },
   "source": [
    "#### RIM-ONE v3\n",
    "\n",
    "Accessing data, preparing train/validation sets division:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaVW0TrDMeTz"
   },
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBIJKCV3sqKQ"
   },
   "source": [
    "### Let's start by testing on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHL_zIZ0MeTz"
   },
   "outputs": [],
   "source": [
    "img_path = \"/content/train_512/5/TRAIN096878.jpg\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "img = np.array(img)\n",
    "t = transf.Compose([\n",
    "    transf.ToTensor(),\n",
    "    transf.Resize([256,256])\n",
    "])\n",
    "img = t(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F4hczeWHMeT0"
   },
   "outputs": [],
   "source": [
    "img_k = tf.keras.preprocessing.image.load_img(\n",
    "    img_path, grayscale=False, color_mode=\"rgb\", target_size=(256,256), interpolation=\"nearest\"\n",
    ")\n",
    "img_k  = img_to_array(img_k)\n",
    "img_k = img_k.astype(np.float64) / 255.0\n",
    "img_k = np.expand_dims(a=img_k,axis=0)\n",
    "img_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "nWPnQCGgMeT0",
    "outputId": "04a6b205-a64b-437c-f607-aad7d21478f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    model = get_unet_light(img_rows=256, img_cols=256)\n",
    "    model.load_weights('/content/drive/MyDrive/airogs/last_checkpoint.hdf5') # you can download the checkpoint file from the original repo(https://github.com/seva100/optic-nerve-cnn)\n",
    "    mask = model.predict(img_k)\n",
    "    print(mask.shape)\n",
    "\n",
    "    plt.imshow(mask[0, 0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "TAyn7bHGMeT0",
    "outputId": "72679229-db59-44d5-ac38-9b2a3eef85df"
   },
   "outputs": [],
   "source": [
    "print(img_path)\n",
    "im = np.array(Image.open(img_path))\n",
    "\n",
    "#Image\n",
    "im = cv2.resize(im, (256,256))\n",
    "im = im.astype(np.float64) / 255.0\n",
    "im = skimage.exposure.equalize_adapthist(im)\n",
    "plt.imshow(im), plt.show()\n",
    "\n",
    "#Predicted Image\n",
    "im = np.expand_dims(im, axis=0)\n",
    "im = tf_to_th_encoding(im)\n",
    "OwnPred = (model.predict(im)[0, 0]).astype(np.float64)\n",
    "plt.imshow(OwnPred, cmap=plt.cm.Greys_r), plt.show()\n",
    "OwnPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-7OYSc7_2_U",
    "outputId": "0deb6196-3928-4184-9cc3-903afbeccd7d"
   },
   "outputs": [],
   "source": [
    "mask = torch.Tensor(OwnPred)\n",
    "mask[mask > 0.35] = 1.0\n",
    "mask[mask <= 0.35] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCnt9MpjkYiw"
   },
   "outputs": [],
   "source": [
    "from imgaug.augmenters.meta import Augmenter\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from imgaug.augmentables.bbs import BoundingBoxesOnImage\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "# We get the unique colors, as these would be the object ids.\n",
    "obj_ids = torch.unique(mask)\n",
    "\n",
    "# first id is the background, so remove it.\n",
    "obj_ids = obj_ids[1:]\n",
    "\n",
    "# split the color-encoded mask into a set of boolean masks.\n",
    "# Note that this snippet would work as well if the masks were float values instead of ints.\n",
    "masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "\n",
    "boxes = masks_to_boxes(masks)\n",
    "print(boxes.shape)\n",
    "print(boxes)\n",
    "\n",
    "pad_x = (boxes[0][2] - boxes[0][0]) * 0.3\n",
    "pad_y = (boxes[0][3] - boxes[0][1]) * 0.3\n",
    "\n",
    "pad = max(pad_x, pad_y)\n",
    "pad = max(pad, 20)\n",
    "print(pad)\n",
    "\n",
    "x1 = max(0, boxes[0][0] - pad)\n",
    "x2 = min(255, boxes[0][2] + pad)\n",
    "y1 = max(0, boxes[0][1] - pad)\n",
    "y2 = min(255, boxes[0][3] + pad)\n",
    "\n",
    "\n",
    "boxes = [[x1, y1, x2, y2]]\n",
    "print(boxes)\n",
    "boxes = torch.Tensor(boxes)\n",
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "8bjy186clzX7",
    "outputId": "e9a1ccfb-10b6-447f-9bf3-4b7ec57ccd33"
   },
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "ASSETS_DIRECTORY = \"assets\"\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img), cmap=\"gray\")\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "\n",
    "drawn_boxes = draw_bounding_boxes(torch.Tensor(im[0] * 255).type(torch.ByteTensor), boxes, colors=\"pink\")\n",
    "show(drawn_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49Z4LvntX4x"
   },
   "source": [
    "### Now, we can apply this algorithm to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ebnL03F2hQk",
    "outputId": "c0566b68-63f6-4312-804b-6697fe6bb2a9"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_csv = pd.read_csv(\"/content/drive/MyDrive/airogs/train.csv\")\n",
    "val_csv = pd.read_csv(\"/content/drive/MyDrive/airogs/val.csv\")\n",
    "\n",
    "\n",
    "no_bounding = 0\n",
    "with_bounding = 0\n",
    "\n",
    "\n",
    "train_csv[\"x1\"] = None\n",
    "train_csv[\"y1\"] = None\n",
    "train_csv[\"x2\"] = None\n",
    "train_csv[\"y2\"] = None\n",
    "val_csv[\"x1\"] = None\n",
    "val_csv[\"y1\"] = None\n",
    "val_csv[\"x2\"] = None\n",
    "val_csv[\"y2\"] = None\n",
    "\n",
    "def show(imgs, title):\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            img = F.to_pil_image(img)\n",
    "            axs[0, i].imshow(np.asarray(img), cmap=\"gray\")\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "            axs[0, i].set_title(title)\n",
    "\n",
    "def crop_image_only_outside(img,tol=20):\n",
    "    # img is 2D or 3D image data\n",
    "    # tol  is tolerance\n",
    "    mask = img > tol\n",
    "    if img.ndim == 3:\n",
    "        mask = mask.all(2)\n",
    "    m,n = mask.shape\n",
    "    mask0,mask1 = mask.any(0),mask.any(1)\n",
    "    col_start, col_end = mask0.argmax(), n - mask0[::-1].argmax()\n",
    "    row_start, row_end = mask1.argmax(), m - mask1[::-1].argmax()\n",
    "    \n",
    "    return img[row_start : row_end, col_start : col_end]\n",
    "\n",
    "for i, row in tqdm(train_csv.iterrows(), total=train_csv.shape[0]):\n",
    "  _img_path = glob.glob(\"/content/train_512/*/\" + row[\"challenge_id\"] + \".jpg\")\n",
    "  if len(_img_path) == 0:\n",
    "    continue\n",
    "    \n",
    "  im = np.array(Image.open(_img_path[0]))\n",
    "\n",
    "  #Image\n",
    "  im = cv2.resize(im, (256,256))\n",
    "  im = im.astype(np.float64) / 255.0\n",
    "  im = skimage.exposure.equalize_adapthist(im)\n",
    "\n",
    "\n",
    "  #Predicted Image\n",
    "  im = np.expand_dims(im, axis=0)\n",
    "  im = tf_to_th_encoding(im)\n",
    "  OwnPred = (model.predict(im)[0, 0]).astype(np.float64)\n",
    "\n",
    "  mask = torch.Tensor(OwnPred)\n",
    "  mask[mask > 0.35] = 1.0\n",
    "  mask[mask <= 0.35] = 0.0\n",
    "\n",
    "  # We get the unique colors, as these would be the object ids.\n",
    "  obj_ids = torch.unique(mask)\n",
    "\n",
    "  # first id is the background, so remove it.\n",
    "  obj_ids = obj_ids[1:]\n",
    "\n",
    "  # split the color-encoded mask into a set of boolean masks.\n",
    "  # Note that this snippet would work as well if the masks were float values instead of ints.\n",
    "  masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "  boxes = masks_to_boxes(masks)\n",
    "\n",
    "\n",
    "  if boxes.shape[0] == 1:\n",
    "    pad_x = (boxes[0][2] - boxes[0][0]) * 0.3\n",
    "    pad_y = (boxes[0][3] - boxes[0][1]) * 0.3\n",
    "\n",
    "    pad = max(pad_x, pad_y)\n",
    "    pad = max(pad, 20)\n",
    "\n",
    "    x1 = max(torch.tensor(0), boxes[0][0] - pad)\n",
    "    x2 = min(torch.tensor(255), boxes[0][2] + pad)\n",
    "    y1 = max(torch.tensor(0), boxes[0][1] - pad)\n",
    "    y2 = min(torch.tensor(255), boxes[0][3] + pad)\n",
    "\n",
    "\n",
    "    boxes = [[x1, y1, x2, y2]]\n",
    "    boxes = torch.Tensor(boxes)\n",
    "\n",
    "    width = abs(boxes[0][2] - boxes[0][0])\n",
    "    height = abs(boxes[0][3] - boxes[0][1])\n",
    "    \n",
    "    bb_area = width * height\n",
    "    o_area = im.shape[2] * im.shape[2]\n",
    "\n",
    "\n",
    "  if boxes.shape[0] == 1 and (abs(bb_area - o_area) > (0.65 * o_area)):\n",
    "\n",
    "    ASSETS_DIRECTORY = \"assets\"\n",
    "\n",
    "    plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "    with_bounding += 1\n",
    "\n",
    "    train_csv.loc[i, \"x1\"] = x1.item()\n",
    "    train_csv.loc[i, \"y1\"] = y1.item()\n",
    "    train_csv.loc[i, \"x2\"] = x2.item()\n",
    "    train_csv.loc[i, \"y2\"] = y2.item()\n",
    "\n",
    "  else:\n",
    "    transform = transforms.Compose([\n",
    "      transforms.ToTensor(),                              \n",
    "      transforms.CenterCrop(148)\n",
    "    ])\n",
    "    _img = transform(np.moveaxis(im[0], 0, -1))\n",
    "\n",
    "\n",
    "    no_bounding += 1\n",
    "\n",
    "    train_csv.loc[i, \"x1\"] = -1\n",
    "    train_csv.loc[i, \"y1\"] = -1\n",
    "    train_csv.loc[i, \"x2\"] = -1\n",
    "    train_csv.loc[i, \"y2\"] = -1\n",
    "\n",
    "\n",
    "print(f\"With Bounding count: {with_bounding}\")\n",
    "print(f\"No Bounding count: {no_bounding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEasUi8QTZFg"
   },
   "outputs": [],
   "source": [
    "train_csv.to_csv(\"/content/drive/MyDrive/airogs/train_256_bbs_final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
